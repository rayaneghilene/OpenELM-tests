{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayaneghilene/OpenELM-tests/blob/main/OpenELM_1_1B_Instruct_4bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apple OpenELM 1.1B text generation (4 bit quant)"
      ],
      "metadata": {
        "id": "RKqri4XG7CyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the latest version of transformers from Github"
      ],
      "metadata": {
        "id": "ESdAEZLR7IFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers --progress-bar off\n",
        "!pip install -q datasets loralib sentencepiece --progress-bar off\n",
        "!pip -q install bitsandbytes accelerate xformers einops --progress-bar off"
      ],
      "metadata": {
        "id": "TOyVaq6r3oVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100b61b4-4bbe-4a52-c16c-60f418f377a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NItbG3Ooedqs",
        "outputId": "751200da-06e3-455c-a8a0-baad0a69cbcb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer\n",
        "The OpenELM model family uses the Llama-2-7b Tokenizer, this means"
      ],
      "metadata": {
        "id": "H0shki19igLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "kpXQGhHlij6q"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\",\n",
        "                                          use_auth_token=True,\n",
        "                                          # padding_side='left'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4Xq1at9iixB",
        "outputId": "a498830f-6419-4c8e-930c-b9043851d91b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:758: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenELM-1_1B-Instruct Model\n",
        "\n",
        "We use BitsAndBytes to get quantized Version of the model in 4 bit. This allows us to run the model on GPU poor machines / Colab notebooks"
      ],
      "metadata": {
        "id": "DSOBSkJB-IOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-1_1B-Instruct\",\n",
        "                                             device_map=device,\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             use_auth_token=True,\n",
        "                                             trust_remote_code=True,\n",
        "                                             quantization_config=bnb_config\n",
        "                                             )"
      ],
      "metadata": {
        "id": "7aeSC9Gn-H7D"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation using the model.generate method"
      ],
      "metadata": {
        "id": "SXZncjXz9AJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_prompt(prompt:str):\n",
        "  tokens = tokenizer(prompt)\n",
        "  tokenized_prompt = torch.tensor(\n",
        "        tokens['input_ids'],\n",
        "        device = device\n",
        "    )\n",
        "  return tokenized_prompt.unsqueeze(0)\n",
        "\n",
        "def generate(prompt:str, model:AutoModelForCausalLM, max_length:int = 128):\n",
        "  tokenized_prompt = prepare_prompt(prompt)\n",
        "  output_ids = model.generate(\n",
        "        tokenized_prompt,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=0,\n",
        "        assistant_model = model)\n",
        "  output_text = tokenizer.decode(\n",
        "        output_ids[0].tolist(),\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "  return output_text"
      ],
      "metadata": {
        "id": "DnfbeQ8CU8iV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on custom prompts"
      ],
      "metadata": {
        "id": "TrtN2ZjhZZj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"tell me a story \\n\"\n",
        "print(generate(prompt, model, 300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_qbfhFHVAas",
        "outputId": "d5a5b41a-a7f6-4386-8a9e-b2785c7b2f43"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tell me a story \n",
            "\n",
            "once upon a time, there was a boy named john. john loved to tell stories. his stories were always full of adventure, danger, and heartwarming love.\n",
            "\n",
            "one day, john met a girl named jane. jane loved to listen to john's stories. john's stories were full of magic, mystery, and magic beans.\n",
            "\n",
            "once, john told a story about a magic beanstalk. jane listened intently, and when john finished, jane asked:\n",
            "\n",
            "jane: \"john, can you tell me about the magic beanstalk?\"\n",
            "\n",
            "john: \"yes! the beanstalk is a legendary story about a magical place where a boy found a golden beanstalk and lived happily ever after with his magic beans.\n",
            "\n",
            "john told all about the beanstalk, its dangers, and the boy's adventures. jane listened intently, and when john finished, jane asked:\n",
            "\n",
            "jane: \"john, can you tell me your favorite part about the beanstalk?\"\n",
            "\n",
            "john: \"my favorite part about the beanstalk is when the boy found the golden beanstalk and climbed up it!\"\n",
            "\n",
            "john told jane about how he climbed the beanstalk, and when he reached the top,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"generate the code to rename the class positive to negartive in a pandas dataframe \\n\"\n",
        "generated_text = generate(prompt, model, 500)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qHrby3n0109",
        "outputId": "d5b3e098-f011-42c3-b3e5-b2ae63d7c9ad"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generate the code to rename the class positive to negartive in a pandas dataframe \n",
            "\n",
            "```python\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import re\n",
            "import os\n",
            "import shutil\n",
            "import glob\n",
            "import subprocess\n",
            "\n",
            "def rename_class(path, class_name, new_class_name, overwrite=True):\n",
            "    file_list = glob.glob(path + '/*.' + class_name)\n",
            "    if len(file_list) > 0:\n",
            "        file_to_rename = random_file(file_list)\n",
            "        subprocess.run([\"mv\", file_to_rename, path + new_class_name + \"_\" + class_name + \"_\" + file_to_rename])\n",
            "        print(\"Renamed class: \" + new_class_name)\n",
            "        if overwrite:\n",
            "            os.remove(file_to_rename)\n",
            "    return file_to_rename\n",
            "\n",
            "def random_file(file_list):\n",
            "    return \".\".join(random_string(len(file_list)), class_name=class_name=\"rand_file\")\n",
            "\n",
            "def class_exists(path):\n",
            "    return os.path.exists(path) and os.path.isfile(path)\n",
            "\n",
            "def rename_classes(path, classes):\n",
            "    file_list = glob.glob(path + '/*.' + classes)\n",
            "    if len(file_list) > 0:\n",
            "        file_to_rename = random_file(file_list)\n",
            "        subprocess.run([\"mv\", file_to_rename, path + classes + \"_\" + classes + \"_\" + file_to_rename])\n",
            "        print(\"Renamed classes: \" + classes)\n",
            "        classes_to_remove = classes.split(\"_\")[1:]\n",
            "        for cls in classes_to_remove:\n",
            "            if class_exists(path + cls):\n",
            "                shutil.rmtree(path + cls)\n",
            "        os.rmdir(path + classes)\n",
            "    return file_to_rename\n",
            "\n",
            "def rename_classes_and_files(path, classes, files):\n",
            "    file_list = glob.glob(path + '/*.' + classes)\n",
            "    if len\n",
            "CPU times: user 32.5 s, sys: 93.3 ms, total: 32.6 s\n",
            "Wall time: 36.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation using the huggingface Pipeline"
      ],
      "metadata": {
        "id": "7eYf4LTa8udd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "text_gen_pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer= tokenizer,\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map=device,\n",
        "                do_sample=True,\n",
        "                pad_token_id=0,\n",
        "                top_k=300,\n",
        "                max_length=300,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                trust_remote_code=True,\n",
        "                )"
      ],
      "metadata": {
        "id": "fvuuA6wF1JH7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "text_gen_pipe(\"tell me a story\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIBjQlbNW-L4",
        "outputId": "606d4350-5753-442d-bfdc-8d4c035f78be"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 22.8 s, sys: 51.4 ms, total: 22.8 s\n",
            "Wall time: 25.6 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'tell me a story growth DNA�� communication similar notifications through notificationsaked elections earlier� brokeniringgediring impressionings patientsainsNG sentences��N off ng no� previously� recent soon couwn from posts your Ng�� time collection. as recently compared recently similar premences proven ab�anceured� throughoutns� inches� shared thoseN online��N    understanding soon standard�NSNS kiles� communicationavingns�Nov non�ings��omcol� understandingriersns patients recentlyains�nanningsologiesainscN��� deathrelings�FFFFDIingsabs network timegoing�� n��izations abE� sains growthNgoingestions experiences recent no� NS administration ab network ra�ones notifications    electricings girlonesanks��R decision files ultimately reports� continues inches running      � notifications has�ainsN owner� dil�� sids� noNOns value NigerNOhnFF soonNNSays�Updateditting few� noistsح maintainays police electionsNAlo�� navigation��N levels non�� agreementSestions those cons going inc agreement� reviews    navigation death� networks has hands���� eventually those memlications concerns�ishment control�� electriciringieves death communication smna constant kices�ns administration posts�NON�ن�NO���� charges��ح departure death� n�Novbers recentlyiles DNA processes evolution continues�� lossienceays�'}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The model seem to perfom Better when using the .generate method compared to the of hugging face 'text-generation' pipeline**"
      ],
      "metadata": {
        "id": "oYAyn9_rZbTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "The performance of the model fell short of expectations. This underscores the ongoing need for enhancements and refinements to ensure optimal functionality and effectiveness in various tasks and contexts.\n",
        "\n",
        "Contact me at rayane.ghilene@ensea.fr if you have any questions."
      ],
      "metadata": {
        "id": "KTJHDsrA9Eyc"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}